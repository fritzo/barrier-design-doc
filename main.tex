%%%% Small single column format
\documentclass[anonymous=false, %
               format=acmsmall, %
               review=true, %
               screen=true, %
               nonacm=true]{acmart}

\usepackage{fancyvrb}
\usepackage[ruled]{algorithm2e} 
%\usepackage{parskip}

\urlstyle{tt}
\citestyle{acmauthoryear}

\begin{document}

\title{Delayed Sampling via Funsors and Barriers}
%  \titlenote{This is a titlenote}
%  \subtitle{This is a subtitle}
%  \subtitlenote{Subtitle note}

\author{Fritz Obermeyer}
%\orcid{1234-5678-9012-3456}
\affiliation{%
  \institution{Uber AI}
  %\department{}
  %\streetaddress{43 Vassar St}
  %\city{Cambridge}
  %\state{MA}
  %\postcode{02139}
  %\country{USA}
}
\email{fritzo@uber.com}

\author{Eli Bingham}
%\orcid{1234-5678-9012-3456}
\affiliation{%
  \institution{Uber AI}
  %\streetaddress{625 Mt Auburn St #3}
  %\city{Cambridge}
  %\state{MA}
  %\postcode{02138}
  %\country{USA}
}
\email{eli.bingham@uber.com}
%\renewcommand\shortauthors{Mage, M. et al}

\begin{abstract}
Delayed sampling is an inference technique for automatic Rao-Blackwellization in sequential latent variable models.
Funsors are a software abstraction generalizing Tensors and Distributions and supporting seminumerical computation including analytic integration.
We demonstrate how to easily implement delayed sampling in an embedded probabilistic programming language using Funsors and effect handlers for \texttt{sample} statements and a \texttt{barrier} statement.
\end{abstract}

\maketitle

\section{Introduction}

Recently interest has grown in techniques to implement light-weight probabilistic programming languages (PPLs) as embedded domain specific languages (DSLs) in other popular languages used in industry, e.g. Figaro \cite{pfeffer2009figaro} and Ranier \cite{bryant2018ranier} in Scala, Edward \cite{tran2017deep} in Python+Tensorflow, Pyro \cite{bingham2018pyro} in Python+PyTorch, Infergo \cite{tolpin2019deployable} in Go, and the PPX protocol \cite{baydin2019efficient} for integrating Python+PyTorch+SHERPA.
Among approaches to lightweight embedded PPLs, effect handlers have shown promise \cite{moore2018effect,pretnar2015introduction}, allowing PPLs like Pyro and Edward2 \cite{tran2018simple} to apply program transformations without needing program analysis or even compilation.
Here we show that delayed sampling can also be implemented in a light-weight embedded PPL using only a \verb$barrier$ statement and limited support from an underlying math library.

\section{Delayed sampling}

Let us distinguish two types of inference strategies in probabilistic programming, call them \emph{lazy} and \emph{eager}.
Let us say a strategy is \emph{lazy} if it it first symbolically evaluates or compiles model code, then globally analyzes the code to create an inference update strategy.
Say a strategy \emph{eager} if it eagerly executes model code, drawing samples from each latent variable.
For example the autograd-based inference algorithms in Pyro are eager in the sense that samples are eagerly created at each sample site.

It is often advantageous to combine lazy and eager strategies, performing local exact computations within small parts of a probabilistic model, but drawing samples to communicate between those parts.
Examples include Rao-Blackwellized SMC filters and their generalization as implemented in Birch \cite{murray2017delayed}, and reactive probabilistic programming \cite{baudart2019reactive}.

This work addresses the challenge of implementing boundedly-lazy inference in a lightweight embedded PPL where samples are eagerly drawn and control flow may depend on those sample values.
Our approach is to use Funsors, a software abstraction generalizing Tensors, Distributions, and lazy compute graphs.
The core idea is to allow lazy sample statements during program execution, and to trigger sampling of lazy random variables only at user-specified \verb$barrier$ statements, typically either immediately before control flow or immediately after a variable goes out of scope.

\section{Embedded probabilistic programming languages with effects}

Consider an embedded probabilistic programming language, extending a host language by adding two primitive statements:
\begin{itemize}
  \item The statement \verb$x = sample(name,dist)$ is a named stochastic statement, where \verb$x$ is a Tensor or Funsor value, \verb$name$ is a unique identifier for the statement, and \verb$dist$ is a distribution (possibly a Funsor).
  \item The statement \verb$state = barrier(state)$ eliminates any free/delayed variables from the recursive observations structure \verb$state$, which may contain Tensor or Funsor values (we restrict attention to lists of Tensors/Funsors).
\end{itemize}
We use Python for the host language in this paper.

We will implement each inference algorithm as a single effect handler.
Each inference algorithm will input observed observations, allow running of model code, and can then interpret nonstandard model outputs as posterior probability distributions, e.g.
\begin{Verbatim}[samepage=true]
with MyInferenceAlgorithm(observations=observations) as inference:
    output = model()  # executes with nonstandard interpretation

posterior = inference.get_posterior(output)
\end{Verbatim}
where \verb$observations$ is a dictionary mapping sample statement name to observed value, and the resulting \verb$posterior$ is some representation of the posterior distribution over latent variables, e.g. an importance-weighted bag of samples.

\section{Inference in sequential models}

Consider a model of a stochastic control system with piecewise control, attempting to keep a latent state \verb$z$ within the interval $[-10,10]$
\begin{Verbatim}[samepage=true]
 1  def model():
 2      z = sample("z_init",Normal(0,1))  # latent state
 3      k = 0                             # control
 4      cost = 0                          # cumulative cost of controller
 5      for t in range(1000):
 6          if z > 10:                    # control flow depends on z
 7              k -= 1
 8          elif z < -10:
 9              k += 1
10          else:
11              k = 0
12          cost += abs(k)
13          z = sample(f"z_{t}",Normal(z+k,1))
14          x = sample(f"x_{t}",Normal(z,1))
15      return cost
\end{Verbatim}
Now suppose we want to estimate the total controller cost given a sequence of observations \verb$x$.
One approach to inference is Sequential Monte Carlo (SMC) filtering.
To maintain a vectorized population of particles we can rewrite the model using a vectorized conditional (e.g. \verb$where(cond,if_true,if_false)$ as implemented in NumPy and PyTorch).
Further we can support resampling of particle populations by adding a \verb$barrier$ statement to the model code; this is needed to communicate resampling decisions to the model's local state.
\begin{Verbatim}[samepage=true]
 1  def model():
 2      z = sample("z_init",Normal(0,1))    # latent state
 3      k = 0 * z                           # control
 4      cost = 0 * z                        # cumulative cost of controller
 5      for t in range(1000):
 6          z,k,cost = barrier([z,k,cost])  # inference may resample here
 7          k = where(z > 10, k + 1, k)
 8          k = where(z < 10, k + 1, k)
 9          k = where(-10 <= z & z <= 10, 0 * k, k)
10          z = sample(f"z_{t}",Normal(z+k,1))
11          x = sample(f"x_{t}",Normal(z,1))
12      return cost
\end{Verbatim}
See appendix \ref{sec:appendix:smc} for details of the effect handler to implement SMC inference.

Notice that if there were no control \verb$k$ (or indeed if the control were linear), we could completely Rao-Blackwellize using a Kalman filter: inference via variable elimination would be exact.
To implement linear-time exact inference would require only:
\begin{itemize}
  \item lazy versions of tensor operations such as \verb$where$;
  \item a lazy interpretation for \verb$sample$ statements;
  \item a variable-eliminating computation of the final \verb$log_joint$ latent state.
\end{itemize}
See appendix \ref{sec:appendix:exact} for details of the effect handler to implement variable elimination.
This is the approach taken by Pyro's discrete enumeration inference, which leverages broadcasting in the host tensor DSL to simulate lazy sampling and lazy tensor ops.

To implement delayed sampling, and thereby partially Rao-Blackwellize our SMC inference, we can combine the two above approaches, emitting lazily sampled values from \verb$sample$ statements and eagerly sampling delayed samples at \verb$barrier$ statements, relying on a variable elimination engine to efficiently draw random samples from the partial posterior.
In contrast to the variable-elimination interpretation of \verb$barrier$, this interpretation guarantees all local state is ground and hence can be inspected by conditionals like \verb$where$.
See appendix \ref{sec:appendix:delayed} for details of effect handler to implement delayed sampling.

\section{Conclusion}

We demonstrated flexible inference algorithms in an embedded probabilistic programming language with a new \verb$barrier$ statement and support for lazy computations represented as Funsors.
While Funsor implementations are few, this same technique can be used for delayed sampling of discrete latent variables using only a Tensor library and a variable elimination engine such as \verb$opt_einsum$ \cite{smith2018opt_einsum}.

% See Lawrence's paper for examples including
% * linear+nonlinear Gaussian state space model and
% * Beta-Binomial-Poisson vector-borne disease model
% https://arxiv.org/pdf/1708.07787.pdf

\bibliographystyle{acm-reference-format}
\bibliography{main}

\section{Appendix: Details of effect handling}

\subsection{Effect handling framework}
Before describing effect implementations, we provide a simple framework for effect handling embedded in Python.
Let's start with a standard interpretation, implemented as an effect handler base class.
\begin{Verbatim}[samepage=true]
  class StandardHandler:
      def __enter__(self):
          # install this handler at the beginning of each with statement
          global HANDLER
          self.old_handler = HANDLER
          HANDLER = self
          return self

      def __exit__(self, type, value, traceback):
          # revert this handler at the end of each with statement
          global HANDLER
          HANDLER = self.old_handler

      def sample(self, name, dist):
          return dist.sample()  # by default, draw a random sample

      def barrier(self, state):
          return state  # by default do nothing

  HANDLER = StandardHandler()
\end{Verbatim}
Next we can define user facing statements with late binding to the active effect handler.
\begin{Verbatim}[samepage=true]
  def sample(name, dist):
      return HANDLER.sample(name, dist)

  def barrier(state):
      return HANDLER.barrier(state)
\end{Verbatim}

\subsection{Effect handlers for inference via Sequential Monte Carlo}
\label{sec:appendix:smc}
We can now implement Sequential Monte Carlo inference by maintaining a vector \verb$log_joint$ of particle log weights, sampling independently each particle at \verb$sample$ statements, and resampling at \verb$barrier$ statements.
\begin{Verbatim}[samepage=true]
  class SMC(StandardHandler):
      def __init__(self, observations, num_particles=100):
          self.observations = observations
          self.log_joint = zeros(num_particles)
          self.num_particles = num_particles
  
      def sample(self, name, dist):
          if name in self.observations:
              value = self.observations[name]
          else:
              value = dist.sample(sample_shape=self.log_joint.shape)
          self.log_joint += dist.log_prob(self.observations[name])
          return value
  
      def barrier(self, state):
          index = Categorical(logits=self.log_joint).sample()
          self.log_joint[:] = 0
          state = [x[index] for x in state]
          return state
  
      def get_posterior(self, value):
          probs = exp(self.log_joint)
          probs /= probs.sum()
          return {"samples": value, "probs": probs}
\end{Verbatim}

\subsection{Effect handlers for exact inference via Variable Elimination}
We can implement variable elimination by leveraging lazy compute graphs and exact forward-backward computation of the Funsor library.
This handler ignores barrier statements.
\label{sec:appendix:exact}
\begin{Verbatim}[samepage=true]
  class VariableElimination(StandardHandler):
      def __init__(self, observations, num_particles=100):
          self.observations = observations
          self.log_joint = funsor.Number(0)
          self.num_particles = num_particles

      def sample(self, name, dist):
          if name in self.observations:
              value = self.observations[name]
          else:
              value = funsor.Variable(name)  # create a delayed sample
          self.log_joint += dist.log_prob(self.observations[name])
          return value

      def get_posterior(self, value):
          return funsor.Expectation(log_joint, value)
\end{Verbatim}

\subsection{Effect handlers for inference via Delayed Sampling}
Finally we can implement delayed sampling by extending the \verb$VariableElimination$ handler to eagerly eliminate variables whenever a barrier statement is encountered.
\label{sec:appendix:delayed}
\begin{Verbatim}[samepage=true]
  class DelayedSampling(VariableElimination):
      def barrier(self, state):
          subs = self.log_joint.sample(state.inputs, self.num_samples)
          self.log_joint = self.log_joint(**subs)
          state = [x(**subs) for x in state]
          return state
\end{Verbatim}

\end{document}
